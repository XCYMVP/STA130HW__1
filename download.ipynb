{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ab655b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1: Pick one of the datasets from the ChatBot session(s) of the TUT demo (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14521565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 391 rows and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "\n",
    "#(1) use code provided in your ChatBot session to print out the number of rows and columns of the dataset.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset using pandas\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "villagers = pd.read_csv(url)\n",
    "\n",
    "# Get the number of rows and columns\n",
    "rows, columns = villagers.shape\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"The dataset contains {rows} rows and {columns} columns.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(2) write your own general definitions of the meaning of \"observations\" and \"variables\" based on asking the ChatBot to explain these terms in the context of your dataset\n",
    "\n",
    "# Definiation of \"observations\": Observations are objects being observed, the investigation targets, the data collection targets. Usually each observation corresponds to a row in a data table.\n",
    "# Definiation of \"Variables\" : Variables are the features and characteristics of observations, or the types of observations.Usually variables correspond to columns in a data table. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46dbd46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Numerical Columns:\n",
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n",
      "\n",
      "Count of Unique Values in the 'species' Column:\n",
      "species\n",
      "cat          23\n",
      "rabbit       20\n",
      "frog         18\n",
      "squirrel     18\n",
      "duck         17\n",
      "dog          16\n",
      "cub          16\n",
      "pig          15\n",
      "bear         15\n",
      "mouse        15\n",
      "horse        15\n",
      "bird         13\n",
      "penguin      13\n",
      "sheep        13\n",
      "elephant     11\n",
      "wolf         11\n",
      "ostrich      10\n",
      "deer         10\n",
      "eagle         9\n",
      "gorilla       9\n",
      "chicken       9\n",
      "koala         9\n",
      "goat          8\n",
      "hamster       8\n",
      "kangaroo      8\n",
      "monkey        8\n",
      "anteater      7\n",
      "hippo         7\n",
      "tiger         7\n",
      "alligator     7\n",
      "lion          7\n",
      "bull          6\n",
      "rhino         6\n",
      "cow           4\n",
      "octopus       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Count of Unique Values in the 'personality' Column:\n",
      "personality\n",
      "lazy      60\n",
      "normal    59\n",
      "cranky    55\n",
      "snooty    55\n",
      "jock      55\n",
      "peppy     49\n",
      "smug      34\n",
      "uchi      24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Q3 Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset\n",
    "\n",
    "# df.describe(): This function provides summary statistics for all numerical columns in the dataset, such as count, mean, standard deviation, min, max, and percentiles (for columns like row_n).\n",
    "# df['column'].value_counts(): This function is used to count the occurrences of unique values in a specific column. In this case, we are counting the unique values in the 'species' and 'personality' columns to see how many different types of species and personalities exist and how frequently they appear.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Example 1: Using df.describe() to get summary statistics for numerical columns\n",
    "print(\"Summary Statistics for Numerical Columns:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Example 2: Using df['column'].value_counts() to count occurrences of unique values in the 'species' column\n",
    "print(\"\\nCount of Unique Values in the 'species' Column:\")\n",
    "print(df['species'].value_counts())\n",
    "\n",
    "# Example 3: Using df['column'].value_counts() to count occurrences of unique values in the 'personality' column\n",
    "print(\"\\nCount of Unique Values in the 'personality' Column:\")\n",
    "print(df['personality'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9472ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset (rows, columns): (891, 15)\n",
      "\n",
      "Missing values in numeric columns:\n",
      " survived      0\n",
      "pclass        0\n",
      "age         177\n",
      "sibsp         0\n",
      "parch         0\n",
      "fare          0\n",
      "dtype: int64\n",
      "\n",
      "Summary statistics for numeric columns:\n",
      "          survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
      "\n",
      "Summary statistics for all columns (including non-numeric):\n",
      "           survived      pclass   sex         age       sibsp       parch  \\\n",
      "count   891.000000  891.000000   891  714.000000  891.000000  891.000000   \n",
      "unique         NaN         NaN     2         NaN         NaN         NaN   \n",
      "top            NaN         NaN  male         NaN         NaN         NaN   \n",
      "freq           NaN         NaN   577         NaN         NaN         NaN   \n",
      "mean      0.383838    2.308642   NaN   29.699118    0.523008    0.381594   \n",
      "std       0.486592    0.836071   NaN   14.526497    1.102743    0.806057   \n",
      "min       0.000000    1.000000   NaN    0.420000    0.000000    0.000000   \n",
      "25%       0.000000    2.000000   NaN   20.125000    0.000000    0.000000   \n",
      "50%       0.000000    3.000000   NaN   28.000000    0.000000    0.000000   \n",
      "75%       1.000000    3.000000   NaN   38.000000    1.000000    0.000000   \n",
      "max       1.000000    3.000000   NaN   80.000000    8.000000    6.000000   \n",
      "\n",
      "              fare embarked  class  who adult_male deck  embark_town alive  \\\n",
      "count   891.000000      889    891  891        891  203          889   891   \n",
      "unique         NaN        3      3    3          2    7            3     2   \n",
      "top            NaN        S  Third  man       True    C  Southampton    no   \n",
      "freq           NaN      644    491  537        537   59          644   549   \n",
      "mean     32.204208      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "std      49.693429      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "min       0.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "25%       7.910400      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "50%      14.454200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "75%      31.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "max     512.329200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "\n",
      "       alone  \n",
      "count    891  \n",
      "unique     2  \n",
      "top     True  \n",
      "freq     537  \n",
      "mean     NaN  \n",
      "std      NaN  \n",
      "min      NaN  \n",
      "25%      NaN  \n",
      "50%      NaN  \n",
      "75%      NaN  \n",
      "max      NaN  \n",
      "\n",
      "Non-numeric columns in the dataset: Index(['sex', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Q4 If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by df.shape and what is reported by df.describe() with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column\n",
    "\n",
    "#Summary of Discrepancies\n",
    "#Number of Columns: df.shape includes all columns (numeric and non-numeric), while df.describe() only includes numeric columns unless specified otherwise.\n",
    "#Count in df.describe(): The \"count\" in df.describe() reflects the number of non-missing values in each numeric column, which may be less than the total number of rows reported by df.shape if there are missing values.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check the shape of the dataset (total number of rows and columns)\n",
    "print(\"Shape of the dataset (rows, columns):\", df.shape)\n",
    "\n",
    "# Check for missing values in numeric columns\n",
    "print(\"\\nMissing values in numeric columns:\\n\", df.select_dtypes(include=['number']).isnull().sum())\n",
    "\n",
    "# Use df.describe() to get summary statistics of numeric columns only\n",
    "print(\"\\nSummary statistics for numeric columns:\\n\", df.describe())\n",
    "\n",
    "# Use df.describe(include='all') to show summary statistics for all columns (both numeric and non-numeric)\n",
    "print(\"\\nSummary statistics for all columns (including non-numeric):\\n\", df.describe(include='all'))\n",
    "\n",
    "# Get a list of non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=['number']).columns\n",
    "print(\"\\nNon-numeric columns in the dataset:\", non_numeric_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5df74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5 Use your ChatBot session to help understand the difference between the following and then provide your own paraphrasing summarization of that difference\n",
    "\n",
    "#Attribute: an attribute is a feature of an object, it describes the object. It is usually a static value or a piece of data associated with the object.\n",
    "#Method: a method is a function that is associated with an object. It includes calculations, operations and actions based on the data of the function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457bbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of numeric columns:\n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
      "Summary statistics of numeric columns:\n",
      "         survived      pclass         age       sibsp       parch        fare\n",
      "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
      "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
      "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
      "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
      "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
      "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
      "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n",
      "Number of non-missing values in each column:\n",
      "survived       891\n",
      "pclass         891\n",
      "sex            891\n",
      "age            714\n",
      "sibsp          891\n",
      "parch          891\n",
      "fare           891\n",
      "embarked       889\n",
      "class          891\n",
      "who            891\n",
      "adult_male     891\n",
      "deck           203\n",
      "embark_town    889\n",
      "alive          891\n",
      "alone          891\n",
      "dtype: int64\n",
      "\n",
      "Mean (average) of numeric columns:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#MEAN: The average value of the column. It is calculated as the sum of all the values divided by the count of non-missing values.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMean (average) of numeric columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#Standard Deviation (std): A measure of how spread out the values in the column are from the mean. A higher standard deviation indicates more variability in the data, while a lower standard deviation means the data points are closer to the mean.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStandard deviation of numeric columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11335\u001b[0m, in \u001b[0;36mDataFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11327\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m  11328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  11329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11333\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11334\u001b[0m ):\n\u001b[0;32m> 11335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[1;32m  11337\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:11984\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  11978\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  11979\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11982\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11983\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 11984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  11986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py:11941\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11937\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  11939\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 11941\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  11943\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11204\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11200\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m  11202\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[1;32m  11203\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[0;32m> 11204\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11205\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m  11206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:1459\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1457\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1459\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[1;32m   1462\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/blocks.py:377\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 377\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    380\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:11136\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[1;32m  11135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 11136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    716\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[0;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/numpy/core/_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "#Q6 The df.describe() method provides the 'count', 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Give the definitions (perhaps using help from the ChatBot if needed) of each of these summary statistics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get summary statistics of numeric columns\n",
    "summary_stats = df.describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(\"Summary statistics of numeric columns:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Display the summary statistics\n",
    "print(\"Summary statistics of numeric columns:\")\n",
    "print(summary_stats)\n",
    "\n",
    "\n",
    "#COUNT: The number of non-missing (non-NaN) values in a column. If there are missing values in the dataset, the count will be less than the total number of rows.\n",
    "print(\"Number of non-missing values in each column:\")\n",
    "print(df.count())\n",
    "\n",
    "\n",
    "#MEAN: The average value of the column. It is calculated as the sum of all the values divided by the count of non-missing values.\n",
    "print(\"\\nMean (average) of numeric columns:\")\n",
    "print(df.mean())\n",
    "\n",
    "#Standard Deviation (std): A measure of how spread out the values in the column are from the mean. A higher standard deviation indicates more variability in the data, while a lower standard deviation means the data points are closer to the mean.\n",
    "print(\"\\nStandard deviation of numeric columns:\")\n",
    "print(df.std())\n",
    "\n",
    "#MINIMUM(MIN): The smallest value in the column. This is the lowest data point in the set of values.\n",
    "#MAXIMUM(MAX): The largest value in the column. This is the highest data point in the set of values.\n",
    "print(\"\\nMinimum and Maximum values of numeric columns:\")\n",
    "print(df.min(), df.max())\n",
    "\n",
    "#25% (First Quartile, Q1): The value below which 25% of the data points lie. It represents the first quartile of the data, meaning 25% of the values are smaller than this number.\n",
    "#50% (Median, Q2):The value below which 50% of the data points lie. This is the median of the data, where half of the values are smaller and half are larger.\n",
    "#75% (Third Quartile, Q3):The value below which 75% of the data points lie. It represents the third quartile, meaning 75% of the values are smaller than this number.\n",
    "print(\"\\n25th percentile (Q1), Median (Q2), and 75th percentile (Q3):\")\n",
    "print(df.quantile([0.25, 0.5, 0.75]))\n",
    "\n",
    "\n",
    "#I have tried to find a set of code that runs without TypeError, but seems failed, here are the new codes which ChatBot says correct, but errors still happen:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get summary statistics of numeric columns\n",
    "summary_stats = df.describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(\"Summary statistics of numeric columns:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Filtering only numeric columns for further statistics (mean, std, etc.)\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# 1. Count: Number of non-missing values in each numeric column\n",
    "print(\"\\nNumber of non-missing values in each column:\")\n",
    "print(numeric_df.count())\n",
    "\n",
    "# 2. Mean: Average of numeric columns\n",
    "print(\"\\nMean (average) of numeric columns:\")\n",
    "print(numeric_df.mean())\n",
    "\n",
    "# 3. Standard Deviation (std): Measure of variability\n",
    "print(\"\\nStandard deviation of numeric columns:\")\n",
    "print(numeric_df.std())\n",
    "\n",
    "# 4. Min and Max: Smallest and largest values\n",
    "print(\"\\nMinimum values of numeric columns:\")\n",
    "print(numeric_df.min())\n",
    "print(\"\\nMaximum values of numeric columns:\")\n",
    "print(numeric_df.max())\n",
    "\n",
    "# 5. Quartiles: 25th, 50th (median), and 75th percentiles\n",
    "print(\"\\n25th, 50th (Median), and 75th percentiles of numeric columns:\")\n",
    "print(numeric_df.quantile([0.25, 0.5, 0.75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a8d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after using dropna():\n",
      "    name              email   age  purchase_amount\n",
      "0  Alice  alice@example.com  25.0             50.5\n",
      "DataFrame after deleting the 'email' column:\n",
      "    name   age  purchase_amount\n",
      "0  Alice  25.0             50.5\n",
      "1    Bob  32.0              NaN\n",
      "2   None  28.0             75.0\n",
      "3  David   NaN             30.0\n",
      "DataFrame after deleting the 'social_security_number' column:\n",
      "      name                email  age\n",
      "0    Alice    alice@example.com   25\n",
      "1      Bob      bob@example.com   32\n",
      "2  Charlie  charlie@example.com   28\n",
      "3    David    david@example.com   45\n",
      "DataFrame after using dropna():\n",
      "      name                email  age\n",
      "0    Alice    alice@example.com   25\n",
      "1      Bob      bob@example.com   32\n",
      "2  Charlie  charlie@example.com   28\n",
      "3    David    david@example.com   45\n",
      "Before cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   survived     891 non-null    int64  \n",
      " 1   pclass       891 non-null    int64  \n",
      " 2   sex          891 non-null    object \n",
      " 3   age          714 non-null    float64\n",
      " 4   sibsp        891 non-null    int64  \n",
      " 5   parch        891 non-null    int64  \n",
      " 6   fare         891 non-null    float64\n",
      " 7   embarked     889 non-null    object \n",
      " 8   class        891 non-null    object \n",
      " 9   who          891 non-null    object \n",
      " 10  adult_male   891 non-null    bool   \n",
      " 11  deck         203 non-null    object \n",
      " 12  embark_town  889 non-null    object \n",
      " 13  alive        891 non-null    object \n",
      " 14  alone        891 non-null    bool   \n",
      "dtypes: bool(2), float64(2), int64(4), object(7)\n",
      "memory usage: 92.4+ KB\n",
      "None\n",
      "\n",
      "Missing values in each column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 712 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   survived     712 non-null    int64  \n",
      " 1   pclass       712 non-null    int64  \n",
      " 2   sex          712 non-null    object \n",
      " 3   age          712 non-null    float64\n",
      " 4   sibsp        712 non-null    int64  \n",
      " 5   parch        712 non-null    int64  \n",
      " 6   fare         712 non-null    float64\n",
      " 7   embarked     712 non-null    object \n",
      " 8   class        712 non-null    object \n",
      " 9   who          712 non-null    object \n",
      " 10  adult_male   712 non-null    bool   \n",
      " 11  embark_town  712 non-null    object \n",
      " 12  alive        712 non-null    object \n",
      " 13  alone        712 non-null    bool   \n",
      "dtypes: bool(2), float64(2), int64(4), object(6)\n",
      "memory usage: 73.7+ KB\n",
      "None\n",
      "\n",
      "Missing values in each column after cleaning:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Rows before cleaning: 891\n",
      "Rows after cleaning: 712\n"
     ]
    }
   ],
   "source": [
    "#I see in the instructions you let us to open a new ChatBot window from here, but I choose not to because put them all together is easier for me to review and analyze. I will do it next time if you wish. \n",
    "\n",
    "#Q7: Missing data can be considered \"across rows\" or \"down columns\". Consider how df.dropna() or del df['col'] should be applied to most efficiently use the available non-missing data in your dataset and briefly answer the following questions in your own words\n",
    "\n",
    "#(1):Provide an example of a \"use case\" in which using df.dropna() might be peferred over using del df['col']\n",
    "#Scenario: Working with a dataset containing customer information, including names, emails, ages, and purchase amounts. Some of the rows have missing values in various columns.I want to clean the dataset before analysis by removing rows with any missing data, but you want to retain all columns. In this case, using df.dropna() is preferred over deleting an entire column with del df['col'], which would remove the column entirely from the dataset, even if it only has a few missing values.\n",
    "#Case 1: Using df.dropna()to remove rows where any column contains missing values (NaNs).\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with missing values\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', None, 'David'],\n",
    "    'email': ['alice@example.com', None, 'charlie@example.com', 'david@example.com'],\n",
    "    'age': [25, 32, 28, None],\n",
    "    'purchase_amount': [50.5, None, 75.0, 30.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "print(\"DataFrame after using dropna():\")\n",
    "print(df_cleaned)\n",
    "#Here, df.dropna() removed rows that contained any missing values, leaving only the rows with complete data for all columns.\n",
    "#Case 2: Using del df['col']to remove the column with missing values, lose the entire email column, even though other rows might have valid email addresses.\n",
    "# Delete the 'email' column entirely\n",
    "del df['email']\n",
    "\n",
    "print(\"DataFrame after deleting the 'email' column:\")\n",
    "print(df)\n",
    "\n",
    "#So, this is why we preffer using df.dropna(), it deletes rows with missing value but keeps colume, not like del df['col'] which delete the entire colum even some of the rows have valid data.\n",
    "\n",
    "\n",
    "\n",
    "#(2): Provide an example of \"the opposite use case\" in which using del df['col'] might be preferred over using df.dropna()\n",
    "#Same Scenario as above, but this time I want to remove the useless columns such as a social_security_number column that is mostly empty, In this case, del df['col'] is preferred because it is way easer to delete.\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with a mostly empty 'social_security_number' column\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', 'david@example.com'],\n",
    "    'age': [25, 32, 28, 45],\n",
    "    'social_security_number': [None, None, None, '123-45-6789']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Delete the 'social_security_number' column\n",
    "del df['social_security_number']\n",
    "\n",
    "print(\"DataFrame after deleting the 'social_security_number' column:\")\n",
    "print(df)\n",
    "\n",
    "#Compare to df.fropna()\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "print(\"DataFrame after using dropna():\")\n",
    "print(df_cleaned)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(3):Discuss why applying del df['col'] before df.dropna() when both are used together could be important\n",
    "#This is because it can prevent accident destruction of data and helps preserving as much useful information as possible.If do the other way arround, risk dropping rows that contain missing values only in that column but may have useful information in other columns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#(4):Remove all missing data from one of the datasets you're considering using some combination of del df['col'] and/or df.dropna() and give a justification for your approach, including a \"before and after\" report of the results of your approach for your dataset.\n",
    "#I use the data list of \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Before report: Show the initial dataset summary and missing data counts\n",
    "print(\"Before cleaning:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 1: Delete columns that are mostly irrelevant or contain too many missing values\n",
    "# 'deck' has a large number of missing values and is not essential for analysis\n",
    "del df['deck']\n",
    "\n",
    "# Step 2: Drop rows with any remaining missing values in the remaining columns\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# After report: Show the cleaned dataset summary and missing data counts\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(df_cleaned.info())\n",
    "print(\"\\nMissing values in each column after cleaning:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "# Optional: Display the number of rows before and after cleaning\n",
    "print(f\"\\nRows before cleaning: {len(df)}\")\n",
    "print(f\"Rows after cleaning: {len(df_cleaned)}\")\n",
    "\n",
    "#Step 1: First, identify the columns with missing data and determine which columns are irrelevant or have too many missing values to keep.\n",
    "#Step 2: Secon, Use del df['col'] to remove columns that contain mostly missing data or are not useful for your analysis\n",
    "#Step 3: Third, Use df.dropna() to remove rows with missing values after filtering out unnecessary columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a284931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min   25%   50%   75%   max\n",
      "sex                                                              \n",
      "female  261.0  27.915709  14.110146  0.75  18.0  27.0  37.0  63.0\n",
      "male    453.0  30.726645  14.678201  0.42  21.0  29.0  39.0  80.0\n",
      "        count       mean        std  min       25%      50%   75%       max\n",
      "pclass                                                                     \n",
      "1       216.0  84.154687  78.380373  0.0  30.92395  60.2875  93.5  512.3292\n",
      "2       184.0  20.662183  13.417399  0.0  13.00000  14.2500  26.0   73.5000\n",
      "3       491.0  13.675550  11.778142  0.0   7.75000   8.0500  15.5   69.5500\n",
      "count    714.000000\n",
      "mean      29.699118\n",
      "std       14.526497\n",
      "min        0.420000\n",
      "25%       20.125000\n",
      "50%       28.000000\n",
      "75%       38.000000\n",
      "max       80.000000\n",
      "Name: age, dtype: float64\n",
      "        count       mean        std   min   25%   50%   75%   max\n",
      "sex                                                              \n",
      "female  261.0  27.915709  14.110146  0.75  18.0  27.0  37.0  63.0\n",
      "male    453.0  30.726645  14.678201  0.42  21.0  29.0  39.0  80.0\n"
     ]
    }
   ],
   "source": [
    "#Q8:Give brief explanations in your own words for any requested answers to the questions below\n",
    "#(1):Use your ChatBot session to understand what df.groupby(\"col1\")[\"col2\"].describe() does and then demonstrate and explain this using a different example from the \"titanic\" data set other than what the ChatBot automatically provide for you\n",
    "\n",
    "\n",
    "#use df.groupby(\"sex\")[\"age\"].describe() to demonstrate how this works.\n",
    "#1. df.groupby(\"sex\"): This groups the dataset by the unique values in the sex column (i.e., \"male\" and \"female\").\n",
    "#2. [\"age\"]: We are interested in the age column for each group\n",
    "#3. .describe(): This will provide summary statistics (count, mean, std, min, etc.) for the age column, grouped by sex.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'sex' and describe the statistics for the 'age' column\n",
    "age_grouped_by_sex = df.groupby(\"sex\")[\"age\"].describe()\n",
    "\n",
    "# Display the result\n",
    "print(age_grouped_by_sex)\n",
    "\n",
    "#Another example: df.groupby(\"pclass\")[\"fare\"].describe().\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'pclass' and describe the statistics for the 'fare' column\n",
    "fare_grouped_by_pclass = df.groupby(\"pclass\")[\"fare\"].describe()\n",
    "\n",
    "# Display the result\n",
    "print(fare_grouped_by_pclass)\n",
    "\n",
    "\n",
    "\n",
    "#(2):Assuming you've not yet removed missing values in the manner of question \"7\" above, df.describe() would have different values in the count value for different data columns depending on the missingness present in the original data. Why do these capture something fundamentally different from the values in the count that result from doing something like df.groupby(\"col1\")[\"col2\"].describe()?\n",
    "#The count in df.describe() shows missing data across the whole dataset, while the count in df.groupby(\"col1\")[\"col2\"].describe() shows how missing data is distributed within each group of col1. Both are useful but capture fundamentally different aspects of the data.\n",
    "#Here is a coding example:\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# df.describe() on the whole dataset\n",
    "print(df['age'].describe())\n",
    "\n",
    "# Group by 'sex' and describe the statistics for the 'age' column\n",
    "print(df.groupby(\"sex\")[\"age\"].describe())\n",
    "\n",
    "\n",
    "\n",
    "#(3):Intentionally introduce the following errors into your code and report your opinion as to whether it's easier to (a) work in a ChatBot session to fix the errors, or (b) use google to search for and fix errors: first share the errors you get in the ChatBot session and see if you can work with ChatBot to troubleshoot and fix the coding errors, and then see if you think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT\n",
    "#I have tried to introduce some of the mistakes for both google and chatgpt, I think they both good but I personally prefer chatgpt.This is because I feel it is easier to read and analyze bu using chat, but harder from google. Another possibility for this is that I already used to use chat. \n",
    "\n",
    "#(A)This is the \"Forget to include import pandas as pd in your code\" case:\n",
    "#This is the resbond of chatgpt: \"The error NameError: name 'pd' is not defined occurs when you try to use pd without first importing the pandas library in Python.To resolve this, make sure to include the following import statement at the beginning of your script:\"import pandas as pd\", This will import the pandas library and assign it the alias pd, allowing you to use pd throughout your code.\n",
    "#This is resbond of google: The NameError name 'pd' is not defined occurs when the variable or function pd has not been defined or imported in the current program. To fix this error, the user must either define or import the pd variable or function from its source library.\n",
    "#So they are both efficient and clear, but chatgpt gives a better explaination and tells you how to fix it with an example of coding. \n",
    "\n",
    "#(B): Mistype \"titanic.csv\" as \"titanics.csv\"\n",
    "#resbond of chatgpt: \"Check the File Path: Make sure that the file titanics.csv exists in the correct directory. If it's not in the same folder as your script, you need to provide the full path to the file.\" Then it creates an example and gives solutions step by step to solve it with coding.\n",
    "#resbond of google: Google seems can't handle this, it doesn't give me a foward resbond, I have to scroll the answers that are posted by other people. \n",
    "\n",
    "#(C): Try to use a dataframe before it's been assigned into the variable\n",
    "#resbond of chatgpt:\"The error NameError: name 'df' is not defined means that the variable df has not been defined or assigned any value before being used in your code.To fix this, make sure you have properly created or loaded a DataFrame and assigned it to the variable df before using it.\" Then it gives two examples and tells you how to fix it with coding.\n",
    "#resbond of google:google still doesn't give me a strightforward answer\n",
    "\n",
    "#(D): Forget one of the parentheses somewhere the code\n",
    "#resbond of chatgpt:The SyntaxError: '(' was never closed occurs when you open a parenthesis ( in your code but do not close it with a corresponding closing parenthesis ).Then it gives two way of solving it with examples of coding. \n",
    "#rebond of google: so it seems like google can only answer some simple coding questions strightforwardly, when it encounters a bit more complex ones it would provide you the most related answer provided by other editer. \n",
    "\n",
    "#(E):Mistype one of the names of the chained functions with the code\n",
    "#resbond of chatgpt:The KeyError: 'pclas' occurs when you try to access a column named 'pclas' in a pandas DataFrame, but that column doesn't exist. This can happen due to:Misspelled Column Name and Column Not Present. Then it provides examples of coding as usuall.\n",
    "#resbond of google: it keeps showing result from this website: https://stackoverflow.com/questions/10116518/im-getting-key-error-in-python, I have checked the content, someone asked the question(error) and someone answers it.Not as clear as chatgpt.\n",
    "\n",
    "#I think there is no necceccery to keep trying E and F, the result won't be much different, chatgpt is way more powerful than google search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9:Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?\n",
    "# Mostly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5daed",
   "metadata": {},
   "source": [
    "Here's a summary of our exchanges for your homework assignment:\n",
    "\n",
    "Dataset Exploration:\n",
    "We started by working with a dataset on \"Animal Crossing\" characters. You wanted to inspect the number of rows and columns using Pandas. We discussed the correct methods to load and inspect the data using df.shape and df.describe().\n",
    "Understanding Observations and Variables:\n",
    "I provided simplified definitions for key concepts like \"observations\" and \"variables.\" Observations are the individual records (rows), and variables are the characteristics (columns) recorded for each observation.\n",
    "Working with Summary Statistics:\n",
    "You requested examples of using df.describe() and df['column'].value_counts() in Python, and I demonstrated how these functions provide insights into the dataset by calculating summary statistics and unique value counts for specific columns.\n",
    "Handling Missing and Non-Numeric Data:\n",
    "We discussed how df.describe() behaves when there are missing or non-numeric variables, explaining that it calculates statistics only for non-missing values and for numeric columns.\n",
    "I used the Titanic dataset as an example to explain how df.describe() and df.groupby().describe() differ in how they treat missing values and generate counts.\n",
    "Summary Statistics Definitions:\n",
    "You asked for a breakdown of the summary statistics provided by df.describe() (like count, mean, std, etc.), and I explained each of these metrics using Python code and practical examples.\n",
    "Handling Missing Data:\n",
    "We explored different strategies for handling missing data using df.dropna() and del df['col']. I discussed the use cases where one approach might be preferred over the other.\n",
    "You requested a practical demonstration using the Titanic dataset, and I showed a \"before and after\" example of removing missing data, explaining the reasoning behind the chosen method.\n",
    "Group-Based Summary Statistics:\n",
    "We explored how to generate group-specific summary statistics using df.groupby(\"col1\")[\"col2\"].describe(), and I provided an example grouping the Titanic data by passenger class to show how fares differed between groups.\n",
    "This summary should serve as a comprehensive record of our discussions, which covered various aspects of data analysis using Pandas, including dataset exploration, missing data handling, and summary statistics interpretation. Let me know if you need further details or adjustments!\n",
    "\n",
    "\n",
    "https://chatgpt.com/share/66e3be88-2830-8007-9901-13fd7834f752\n",
    "\n",
    "\n",
    "Here is a summary sepcially for question 3 of Q8:\n",
    "\n",
    "\n",
    "Heres a summary of our conversation:\n",
    "\n",
    "NameError: name 'pd' is not defined:\n",
    "You encountered an error when trying to use pd without importing pandas. I suggested adding import pandas as pd at the beginning of your script to resolve this.\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv':\n",
    "You encountered a file not found error. I advised checking the file path and filename to ensure correctness and provided tips on using absolute vs. relative paths.\n",
    "DF.groupby(\"col1\")[\"col2\"].describe():\n",
    "You provided code to group a DataFrame by one column and describe another. I explained that this code would generate descriptive statistics for col2 based on groups in col1.\n",
    "NameError: name 'df' is not defined:\n",
    "You encountered an error because the DataFrame df wasn't defined. I suggested ensuring that the DataFrame is properly created or loaded before using it.\n",
    "SyntaxError: '(' was never closed:\n",
    "You encountered a syntax error due to an unclosed parenthesis. I recommended checking for missing closing parentheses in your code and provided an example.\n",
    "KeyError: 'pclas':\n",
    "You encountered a KeyError when trying to access a column named 'pclas'. I suggested verifying the column name by printing df.columns, checking for typos, and ensuring the column exists in the dataset.\n",
    "This covers the issues discussed and the corresponding solutions provided during our conversation. Let me know if you'd like any additional details or edits!\n",
    "\n",
    "https://chatgpt.com/share/66e44e13-28b0-8007-87d0-90a652ad899f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
